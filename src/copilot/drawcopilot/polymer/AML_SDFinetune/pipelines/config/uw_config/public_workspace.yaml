# @package _group_
default_compute: cpu-cluster
default_datastore: workspaceblobstore
computes:
  # eastus2:
  - name: fl-eus2
    os: linux
    DC: true
    gpu: false
    location: eastus2
    type: AmlCompute
    environment: mdp
  - name: cpu-cluster
    os: linux
    DC: false
    gpu: false  
    location: eastus2
    type: AmlCompute
    environment: mdp
  - name: data-factory-1  #already deleted, expected to fail
    os: None
    DC: false
    gpu: false
    location: eastus2
    type: DataFactory
    environment: mdp

  # westus2:
  - name: fl-wus2
    os: linux
    DC: true
    gpu: false
    location: westus2
    type: AmlCompute
    environment: mdp
  - name: data-factory
    os: None  #TODO: need to simplify this
    DC: false #TODO: need to simplify this
    gpu: false
    location: westus2
    type: DataFactory
    environment: mdp

datastores:
  # datastores
  - name: fl_eus
    DC: false
    location: eastus2
    environment: mdp
  - name: fl_eus2
    DC: true
    location: eastus2
    environment: mdp
  - name: fl_wus
    DC: false
    location: westus2
    environment: mdp
  - name: fl_wus2
    DC: true
    location: westus2
    environment: mdp

# cosmos
noncompliant_datastore: fl_north_eu2

# data I/O for linux modules
linux_input_mode: download
linux_output_mode: upload

# data I/O for windows modules
windows_input_mode: download
windows_output_mode: upload

# other runsettings
hdi_driver_memory: 2g
hdi_driver_cores: 2
hdi_executor_memory: 2g
hdi_executor_cores: 2
hdi_number_executors: 2
hdi_conf: "{\n  \"spark.yarn.maxAppAttempts\": 2,\n  \"spark.sql.shuffle.partitions\"\
  : 2000,\n  \"spark.yarn.appMasterEnv.PYSPARK_PYTHON\": \"/usr/bin/anaconda/envs/py37/bin/python3\"\
  ,\n  \"spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON\": \"/usr/bin/anaconda/envs/py37/bin/python3\"\
  \n}\n"
synapse_driver_memory: 2g
parallel_node_count: 10
parallel_process_count_per_node: null
parallel_run_invocation_timeout: 10800
parallel_run_max_try: 3
parallel_mini_batch_size: 1
parallel_error_threshold: -1